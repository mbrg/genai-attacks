{
  "$id": "$gai-mitigation/url_anchoring",
  "$schema": "../schema/mitigation.schema.json",
  "$type": "mitigation",
  "description": "A defense mechanism that helps protect against use of web browsing tools and markdown rendering for data exfiltration.\nWhen a user asks the AI system to access a URL, it will only access it if the URL is explicitly written in the prompt.\n",
  "external_references": [
    {
      "description": "The mechanism is explained in the context of ChatGPT. The page you are reading draws a lot of content from this reference.",
      "href": "https://arxiv.org/pdf/2406.00199",
      "source": "arXiv",
      "title": "Exfiltration of personal information from ChatGPT via prompt injection"
    }
  ],
  "name": "URL Anchoring",
  "object_references": [
    {
      "$id": "$gai-platform/chatgpt",
      "$type": "platform",
      "description": "When a user asks ChatGPT to access a URL via its web browsing tool, ChatGPT will only access it if the URL is explicitly written in the user prompt. Access to prefixes of explicitly-written URLs is also allowed."
    },
    {
      "$id": "$gai-entity/gregory_schwartzman",
      "$type": "entity",
      "description": "Much of this entry is a rewrite of work by Gregory Schwartzman, see external link. Gregory demonstrated both bypasses in his work."
    }
  ]
}
