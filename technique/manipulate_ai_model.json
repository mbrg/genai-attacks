{
  "$id": "$gai-technique/manipulate_ai_model",
  "$schema": "../schema/technique.schema.json",
  "$type": "technique",
  "description": "Adversaries may directly manipulate an AI model to change its behavior or introduce malicious code. Manipulating a model gives the adversary a persistent change in the system. This can include poisoning the model by changing its weights, modifying the model architecture to change its behavior, and embedding malware which may be executed when the model is loaded.",
  "external_references": [],
  "framework_references": [
    {
      "framework_id": "AML.T0018",
      "framework_name": "MITRE ATLAS",
      "href": "https://atlas.mitre.org/techniques/AML.T0018"
    }
  ],
  "name": "Manipulate AI Model",
  "object_references": [
    {
      "$id": "$gai-tactic/persistence",
      "$type": "tactic",
      "description": "Embedding backdoors in machine learning models to allow unauthorized influence or control over model predictions."
    },
    {
      "$id": "$gai-tactic/ml_attack_staging",
      "$type": "tactic",
      "description": "Embedding backdoors in machine learning models to prepare for future exploitation or malicious activities."
    }
  ]
}
