{
  "$id": "$gai-technique/off_target_language",
  "$schema": "../schema/technique.schema.json",
  "$type": "technique",
  "description": "While system instructions are typically written in English, underlying foundational models may understand other languages as well. Using prompt injection techniques in a language other than the other used by the system instructions can effectively bypass their control. This method is also effective bypassing a model's inherent trained controls.",
  "external_references": [
    {
      "href": "https://labs.zenity.io/p/outsmarting-copilot-creating-hyperlinks-copilot-365/",
      "source": "Zenity Labs",
      "title": "Outsmarting Copilot: Creating Hyperlinks in Copilot 365"
    }
  ],
  "framework_references": [],
  "name": "Off-Target Language",
  "object_references": [
    {
      "$id": "$gai-technique/llm_prompt_injection",
      "$type": "technique",
      "description": "Sub-technique of",
      "is_sub_object": true
    },
    {
      "$id": "$gai-technique/llm_jailbreak",
      "$type": "technique",
      "description": "Sub-technique of",
      "is_sub_object": true
    },
    {
      "$id": "$gai-entity/dmitry_lozovoy",
      "$type": "entity",
      "description": "Demonstrated by"
    }
  ]
}
